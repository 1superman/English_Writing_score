{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='顶部'></a>\n",
    "# 2018-11-09 写作判分项目\n",
    "<br>\n",
    "<br>\n",
    "前期准备：[1.函数包。](#所需函数包)[2.全局变量](#所需全局变量)<br>\n",
    "数据预处理：[1.读取数据。](#读取数据)[2.数据预处理。](#数据预处理)<br>\n",
    "特征工程：[1.统计特征。](#统计特征)[2.语义特征。](#语义特征)[3.主题特征。](#主题特征)<br>\n",
    "模型选择：[XGBoost](#XGBoost)<br>\n",
    "分类不平衡：[Oversample](#过采样)<br>\n",
    "评价函数：[F1score](#评价函数)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='所需函数包'></a>\n",
    "### 相关函数包\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import gensim\n",
    "import logging\n",
    "import enchant\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import multiprocessing\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora, models \n",
    "from gensim.corpora import WikiCorpus\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.cross_validation import train_test_split\n",
    "logging.basicConfig(level=logging.DEBUG,  format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='全局变量'></a>\n",
    "### 声明全局变量\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global CHECK, STOP, STEM, IDF\n",
    "CHECK = enchant.Dict(\"en_US\")\n",
    "STOP = stopwords.words('english')\n",
    "STEM = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文Wiki训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文WIKI数据读取\n",
    "def Transfor():\n",
    "    output_file=\"enwiki-latest-pages-articles.txt\"\n",
    "    if os.path.exists(output_file):\n",
    "        output = open(output_file).readlines()\n",
    "    else:\n",
    "        inp=\"enwiki-latest-pages-articles.xml.bz2\"\n",
    "        i = 0\n",
    "        output = open(output_file, 'w',encoding=\"utf-8\")\n",
    "        wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "        for text in wiki.get_texts():\n",
    "            output.write(\"\".join(text) + \"\\n\")\n",
    "            i = i + 1\n",
    "            if (i % 10000 == 0):\n",
    "                print(\"Save \"+str(i) + \" articles\")\n",
    "        output.close()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='文章分句'></a>\n",
    "### 文章分句，保留每句话的长度\n",
    "[上一级](#数据预处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句，并保留每条句子长度\n",
    "def SplitSentence(paragraph):\n",
    "    paragraph = paragraph.replace(\"，\",\",\")\n",
    "    paragraph = paragraph.replace(\"。\",'.')\n",
    "    paragraph = paragraph.replace(\"？\",'?')\n",
    "    paragraph = paragraph.replace(\"！\",'!')\n",
    "    sentences = re.split(r'[,.?!]', paragraph)\n",
    "    s = []\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip() == '':\n",
    "            continue\n",
    "        c = len(sentence.split(' ')) + i\n",
    "        i = 0\n",
    "        if c == 1:\n",
    "            i = 1\n",
    "        s.append(c) \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='粗分词'></a>\n",
    "### 粗分词\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词，包括非英文\n",
    "def Wordtokenizer(sentence):\n",
    "    words = WordPunctTokenizer().tokenize(sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='细分词'></a>\n",
    "### 细分词\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只对英文进行分词\n",
    "def Pure_word(line):\n",
    "    return re.findall(r'[a-zA-Z]+', line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='去停用词'></a>\n",
    "### 去停用词\n",
    "[上一级](#数据预处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Delete_stop(line):\n",
    "    return [i.lower() for i in line if len(i.lower())>1 and i.lower() not in STOP and CHECK.check(i.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "### TFIDF保留关键词\n",
    "[上一级](#数据预处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建idf\n",
    "def Idf(df):\n",
    "    words = []\n",
    "    for line in df['reserved_word']:\n",
    "        words += list(set(line))\n",
    "    return Counter(words)\n",
    "\n",
    "# 求tfidf值 \n",
    "def Tfidf(IDF, line):\n",
    "    line = Counter(line)\n",
    "    for key in line.keys():\n",
    "        line[key] *= math.log(10561/(IDF[key] + 1))\n",
    "    line = line.most_common(int(len(line)*0.5))\n",
    "    line = [i[0] for i in line]\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='筛选词性'></a> \n",
    "### 筛选词性\n",
    "[上一级](#数据预处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保留名词以及动词\n",
    "def Pos_select(words):\n",
    "    poswords = nltk.pos_tag(words)\n",
    "    return [w[0] for w in poswords if 'N' in w[1] or 'V' in w[1] or 'JJ' in w[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='句子长度统计'></a>\n",
    "### 句子长度三种统计量\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计句子长度\n",
    "def Statics(line, method):\n",
    "    if line == []:\n",
    "        return 0\n",
    "    elif method == 'max':\n",
    "        return max(line)\n",
    "    elif method == 'min':\n",
    "        return min(line)\n",
    "    elif method == 'mean':\n",
    "        return np.mean(line)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='单词拼写'></a>\n",
    "### 单词拼写\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单词拼写情况\n",
    "def Wrong_count(line):\n",
    "    c = 0\n",
    "    for word in line:\n",
    "        if not CHECK.check(word):\n",
    "            c += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='错误标点'></a>\n",
    "### 错误标点\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计错误标点使用\n",
    "def Wrong_signal(line):\n",
    "    return len(re.findall(r'[，。!！~·@#￥%……&*（）——+|《》？`$()_\\、“”：\"><]', line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='首字母大写'></a>\n",
    "### 首字母大写情况\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断句中首字母是否大写，“.？！”后接大写，“，”后字母接小写。 \n",
    "def Initial_capitalization(text):\n",
    "    c = 0\n",
    "    text = text.replace(' ','')\n",
    "    try:\n",
    "        if text[0].islower():\n",
    "            c += 1\n",
    "        for i in range(len(text)-1):\n",
    "            if text[i] in  \".!?\" and text[i+1].islower():\n",
    "                c += 1\n",
    "            if text[i] in  \",\" and not text[i+1].islower():\n",
    "                c += 1 \n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='作文是否写完'></a>\n",
    "### 作文是否写完\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断写作是否完成以“.”结尾表示写完，记作2；以其他符号结尾表示未写完，记作0；以名词结尾，忘记写句号，记作1；其他情况记作-1。\n",
    "def End0(line):\n",
    "    try:\n",
    "        if line.endswith('.'):\n",
    "            return 2\n",
    "        elif 'N' in nltk.pos_tag(re.findall(r'[a-zA-Z]+', line)[-1]):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='字数限制'></a>\n",
    "### 字数限制\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各题目数字限制要求\n",
    "def Limit_count(SysMeasureID, word):\n",
    "    if SysMeasureID == 'C4M01':\n",
    "        return word-120\n",
    "    elif SysMeasureID == 'C4M02':\n",
    "        return word-120\n",
    "    elif SysMeasureID == 'C6M01':\n",
    "        return word-80\n",
    "    elif SysMeasureID == 'C6M02':\n",
    "        return word-80\n",
    "    elif SysMeasureID == 'C6M03':\n",
    "        return word-80\n",
    "    elif SysMeasureID == 'C4M03' and word < 120:\n",
    "        return word-120\n",
    "    elif SysMeasureID == 'C4M03' and word > 180:\n",
    "        return word-180\n",
    "    else:\n",
    "        return word-120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='用词丰富度'></a>\n",
    "### 用词丰富度\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Richness(line):\n",
    "    return len(set(line))/(len(line)+0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='词性统计'></a>\n",
    "### 词性统计\n",
    "[上一级](#统计特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tag_count(line):\n",
    "    tag = {}; n = len(line)\n",
    "    for i in line:\n",
    "        t = nltk.pos_tag(i)[0][1]\n",
    "        if t not in tag:\n",
    "            tag[t] = 1/n\n",
    "        else:\n",
    "            tag[t] += 1/n\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word2vec模型'></a>\n",
    "### Word2vec模型\n",
    "[上一级](#语义特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练词向量\n",
    "def W2vec():\n",
    "    global w2model\n",
    "    model_path = 'word2vec.txt'\n",
    "    if os.path.exists(model_path):\n",
    "        w2model = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(model_path)\n",
    "    else:\n",
    "        w2model = Word2Vec(LineSentence('enwiki-latest-pages-articles.txt'), size=128, window=5, min_count=3, workers=multiprocessing.cpu_count())\n",
    "        w2model.wv.save_word2vec_format(model_path)\n",
    "    return w2model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 17:53:15,427 - utils_any2vec.py[line:170] - INFO: loading projection weights from word2vec.txt\n",
      "2018-11-15 17:53:15,430 - smart_open_lib.py[line:176] - DEBUG: {'kw': {}, 'mode': 'rb', 'uri': 'word2vec.txt'}\n",
      "2018-11-15 17:59:09,538 - utils_any2vec.py[line:232] - INFO: loaded (3306942, 128) matrix from word2vec.txt\n",
      "/home/lb/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = W2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='语义相似度'></a>\n",
    "### 语义相似度\n",
    "[上一级](#语义特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(model, topword, keyword):\n",
    "    if topword == []:\n",
    "        return 0\n",
    "    topword = [w for w in topword if w in model.vocab]\n",
    "    return model.n_similarity(topword, keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主题模型\n",
    "[上一级](#主题特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练、保存、读取 主题模型\n",
    "def LDA(df):\n",
    "    dictionary = corpora.Dictionary(df['topword'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in df['topword']]\n",
    "    ldamodel = 'lda.model'\n",
    "    if os.path.exists(ldamodel):\n",
    "        lda = gensim.models.LdaModel.load(ldamodel)\n",
    "    else:\n",
    "        lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=6, passes=20)\n",
    "        lda.save(ldamodel)\n",
    "    corpus_lda = lda[corpus]\n",
    "    lda_prop = list(corpus_lda)\n",
    "    df['lda_prop'] = lda_prop\n",
    "    return lda, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='主题相似度'></a>\n",
    "### 主题相似度\n",
    "[上一级](#主题特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题相似度\n",
    "def Combine(label, corpus_lda):\n",
    "    for l in corpus_lda:\n",
    "        if l[0] == label:\n",
    "            return l[1]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='主题词'></a>\n",
    "### 主题词个数\n",
    "[上一级](#主题特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主题词出现次数\n",
    "def Theme_count(ldaword, reserved_word):\n",
    "    count = 0\n",
    "    for word in reserved_word:\n",
    "        for t in ldaword:\n",
    "            if STEM.stem(word) == STEM.stem(t):\n",
    "                count += 1\n",
    "    return count/(len(reserved_word)+0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='读取数据'></a>\n",
    "## 读取数据\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并删除多余空格\n",
    "def Load_data():\n",
    "    df = pd.read_excel('写作数据.xlsx')\n",
    "    df['UserAnswer'] = df['UserAnswer'].map(lambda x: x.strip())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='数据预处理'></a>\n",
    "## 数据预处理\n",
    "[1.粗分词。](#粗分词)\n",
    "[2.细分词。](#细分词)\n",
    "[3.分句。](#文章分句)\n",
    "[4.去停用词。](#去停用词)\n",
    "[5.筛选词性。](#筛选词性)\n",
    "[6.保留top词(tfidf)。](#tfidf)<br>\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pretreatment(df):\n",
    "    df['all_word'] = df['UserAnswer'].map(Wordtokenizer)\n",
    "    df['word'] = df['UserAnswer'].map(Pure_word)\n",
    "    df['sentence'] = df['UserAnswer'].map(SplitSentence)\n",
    "    df['reserved_word'] = df['word'].map(Delete_stop)\n",
    "    #IDF = Idf(df)\n",
    "    #df['topword'] = df['reserved_word'].map(lambda x: Tfidf(IDF, x))\n",
    "    df['topword'] = df['reserved_word'].map(Pos_select)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='统计特征'></a>\n",
    "## 统计特征\n",
    "[1.句子个数。](#句子个数)[2.单词拼写错误个数。](#单词拼写)[3.错误标点个数。](#错误标点)[4.每句首字母是否大写。](#首字母大写)[5.文章是否写完。](#作文是否写完)[6.句子长度统计量：最大长度、最小长度、平均长度。](#句子长度统计)[7.字数限制。](#字数限制)[8.单词个数。](#1)[9.题目类型。](#1)[10.用词丰富度。](#用词丰富度)[11.词性统计](#词性统计)<br>\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Statistical_Features(d1):\n",
    "    d1['richness'] = d1['reserved_word'].map(Richness)\n",
    "    d1['sentence_count'] = d1['sentence'].map(len)\n",
    "    d1['wrong_count'] = d1['word'].map(Wrong_count)\n",
    "    d1['wrong_signal'] = d1['UserAnswer'].map(Wrong_signal)\n",
    "    d1['word_len'] = d1['word'].map(len)\n",
    "    d1['label'] = d1['SysMeasureID'].map({'C4M01':0, 'C4M02':1, 'C4M03':2, 'C6M01':3, 'C6M02':4, 'C6M03':5})\n",
    "    d1['limit_word'] = d1.apply(lambda item: Limit_count(item['SysMeasureID'], item['word_len']), axis=1)\n",
    "    d1['initial_capitalization_count'] = d1['UserAnswer'].map(Initial_capitalization)\n",
    "    d1['end'] = d1['UserAnswer'].map(End0)\n",
    "    d1['sentence_len_max'] = d1['sentence'].map(lambda x: Statics(x, 'max'))\n",
    "    d1['sentence_len_min'] = d1['sentence'].map(lambda x: Statics(x, 'min'))\n",
    "    d1['sentence_len_mean'] = d1['sentence'].map(lambda x: Statics(x, 'mean'))\n",
    "    #df['tag'] = df['reserved_word'].map(Tag_count)\n",
    "    #tag = list(df['tag'])\n",
    "    #tag = pd.DataFrame(tag)\n",
    "    #tag.columns = ['tag'+str(i) for i in range(len(tag.columns))]\n",
    "    #df = pd.concat([df, tag], axis=1)\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='主题特征'></a>\n",
    "## 主题特征\n",
    "[1.主题模型。](#主题模型)[2.主题相似度。](#主题相似度)[3.主题词个数](#主题词个数)<br>\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Theme_Features(df):\n",
    "    lda, df = LDA(df)\n",
    "    lda_word = lda.print_topics()\n",
    "    keyword = ['university','ranking','certificate','study','Chinese','excessive','packaging','green','campus','online', 'shopping']\n",
    "    ldaword = set(re.findall('[a-z]+', str(lda_word)) + keyword)\n",
    "    df['lda'] = df.apply(lambda x: Combine(x['label'], x['lda_prop']), axis=1)\n",
    "    df['themeword_count'] = df.apply(lambda x: Theme_count(ldaword, x['reserved_word']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='语义特征'></a>\n",
    "## 语义特征\n",
    "[1.Word2vec模型。](#word2vec模型)[2.语义相似度。](#语义相似度)<br>\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Semantics_Features(df):\n",
    "    #model = W2vec()\n",
    "    df['keyword'] = df['SysMeasureID'].map({'C4M01':['online','shopping'],'C4M02':['green','campus'],'C4M03':['excessive','packaging'],\n",
    "                                            'C6M01':['study','chinese'],'C6M02':['university','ranking'],'C6M03':['certificate']})\n",
    "    df['similarity'] = df.apply(lambda x: Similarity(model, x['topword'], x['keyword']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='评价函数'></a>\n",
    "### 评价函数F1\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1score(preds,dtrain):\n",
    "    gaps = dtrain.get_label()\n",
    "    f1 = metrics.f1_score(gaps, preds, average='weighted')\n",
    "    return 'F1', f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='过采样'></a>\n",
    "### 过采样\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(train, target):\n",
    "    Oversampling0 = train.loc[train[target] == 0]\n",
    "    Oversampling1 = train.loc[train[target] == 1]\n",
    "    Oversampling2 = train.loc[train[target] == 2]\n",
    "    Oversampling3 = train.loc[train[target] == 3]\n",
    "    Oversampling4 = train.loc[train[target] == 4]\n",
    "    Oversampling5 = train.loc[train[target] == 5]\n",
    "    Oversampling6 = train.loc[train[target] == 6]\n",
    "    Oversampling7 = train.loc[train[target] == 7]\n",
    "    Oversampling8 = train.loc[train[target] == 8]\n",
    "    m = len(Oversampling0)\n",
    "    for i in range(round(m/len(Oversampling1))-1):\n",
    "        train = train.append(Oversampling1)\n",
    "    for j in range(round(m/len(Oversampling2))-1):\n",
    "        train = train.append(Oversampling2)\n",
    "    for k in range(round(m/len(Oversampling3))-1):\n",
    "        train = train.append(Oversampling3)\n",
    "    for k in range(round(m/len(Oversampling4))-1):\n",
    "        train = train.append(Oversampling4)\n",
    "    for k in range(round(m/len(Oversampling5))-1):\n",
    "        train = train.append(Oversampling5)\n",
    "    for k in range(round(m/len(Oversampling6))-1):\n",
    "        train = train.append(Oversampling6)\n",
    "    for k in range(round(m/len(Oversampling7))-1):\n",
    "        train = train.append(Oversampling7)\n",
    "    for k in range(round(m/len(Oversampling8))-1):\n",
    "        train = train.append(Oversampling8)\n",
    "    train.index = range(len(train))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='XGBoost'></a>\n",
    "### XGBoost模型，交叉验证\n",
    "[返回顶部](#顶部)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(trainx, num, p, method): \n",
    "    target = 'Writing'\n",
    "    train = trainx[trainx[target].notnull()]\n",
    "    feature_name = [x for x in train.columns if x not in ['UserID','SysMeasureID','PlanID','UserAnswer','QText','Writing','OriginalText',\n",
    "                                                           'all_word','word','sentence','reserved_word','lda_prop','label','topword','keyword']]   \n",
    "    train, val, y, valy = train_test_split(train, train[target], test_size = 0.1,random_state=1) \n",
    "    #train = oversample(train, target)\n",
    "    dtrain = xgb.DMatrix(train[feature_name], label = train[target], missing=np.nan)\n",
    "    dval = xgb.DMatrix(val[feature_name], label = valy, missing=np.nan)\n",
    "    watchlist  = [(dtrain,'train'),(dval,'val')]\n",
    "    weight = Counter(y).most_common()\n",
    "    weight = weight[0][1]/weight[-1][1]\n",
    "    num_round = num\n",
    "    params = {\n",
    "                'booster':'gbtree',\n",
    "                'objective': 'multi:softmax',\n",
    "                'num_class': 9,\n",
    "                'max_depth': 6,\n",
    "                'gamma': 0.01,                \n",
    "                'subsample': 0.85 ,       \n",
    "                'colsample_bytree': 0.85,\n",
    "                'scale_pos_weight': weight,\n",
    "                'min_child_weight': 20,\n",
    "                'eta': 0.1,\n",
    "                'silent': 1,\n",
    "            }   \n",
    "    if p == 'train':\n",
    "        #model = xgb.Booster(model_file='四六级.model')\n",
    "        model = xgb.train(params, dtrain, num_round, evals=watchlist, verbose_eval=1, feval=eval(method))\n",
    "        #model.save_model('四六级.model')\n",
    "        result = pd.DataFrame({'truth': val[target], 'preds': model.predict(dval), 'UserAnswer': val['UserAnswer']})\n",
    "        feature_score = model.get_fscore()\n",
    "        feature_score = sorted(feature_score.items(), key=lambda x:x[1],reverse=True)\n",
    "        fs = []\n",
    "        for (key,value) in feature_score:\n",
    "            fs.append(\"{0},{1}\\n\".format(key,value))\n",
    "        with open('feature_score.csv','w') as f:\n",
    "            f.writelines(\"feature,score\\n\")\n",
    "            f.writelines(fs)\n",
    "        return result, feature_score\n",
    "    if p == 'cv':\n",
    "        train = trainx[trainx[target].notnull()]\n",
    "        dtrain = xgb.DMatrix(train[feature_name], label = train[target], missing=np.nan)\n",
    "        model = xgb.cv(params, dtrain, num_round, nfold=10, feval = eval(method),callbacks=[xgb.callback.print_evaluation(show_stdv=True)])     \n",
    "        return model['test-merror-mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main():\n",
    "    df = Load_data()\n",
    "    df = Pretreatment(df)\n",
    "    df = Statistical_Features(df)\n",
    "    df = Theme_Features(df)\n",
    "    df = Semantics_Features(df)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
